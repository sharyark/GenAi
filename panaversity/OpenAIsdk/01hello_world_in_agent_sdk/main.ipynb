{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e672a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6608169",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b59b5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tracing disabled\n",
    "# set_tracing_disabled(disabled=True)\n",
    "\n",
    "\n",
    "# # 1. Which LLM Service?\n",
    "# external_client: AsyncOpenAI = AsyncOpenAI(\n",
    "#     api_key=gemini_api_key,\n",
    "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\",\n",
    "# )\n",
    "# # 2. Which LLM Model?\n",
    "# llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     openai_client=external_client\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "572dc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool, set_tracing_disabled\n",
    "\n",
    "BASE_URL = os.getenv(\"EXAMPLE_BASE_URL\") or \"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "API_KEY = os.getenv(\"EXAMPLE_API_KEY\") or \"AIzaSyCKxbxr72MnqaEwogkStGUf5jIjsrTvQVE\"\n",
    "MODEL_NAME = os.getenv(\"EXAMPLE_MODEL_NAME\") or \"gemini-2.0-flash\"\n",
    "\n",
    "if not BASE_URL or not API_KEY or not MODEL_NAME:\n",
    "    raise ValueError(\n",
    "        \"Please set EXAMPLE_BASE_URL, EXAMPLE_API_KEY, EXAMPLE_MODEL_NAME via env var or code.\"\n",
    "    )\n",
    "\n",
    "\"\"\"This example uses a custom provider for a specific agent. Steps:\n",
    "1. Create a custom OpenAI client.\n",
    "2. Create a `Model` that uses the custom client.\n",
    "3. Set the `model` on the Agent.\n",
    "\n",
    "Note that in this example, we disable tracing under the assumption that you don't have an API key\n",
    "from platform.openai.com. If you do have one, you can either set the `OPENAI_API_KEY` env var\n",
    "or call set_tracing_export_api_key() to set a tracing specific key.\n",
    "\"\"\"\n",
    "client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "# An alternate approach that would also work:\n",
    "# PROVIDER = OpenAIProvider(openai_client=client)\n",
    "# agent = Agent(..., model=\"some-custom-model\")\n",
    "# Runner.run(agent, ..., run_config=RunConfig(model_provider=PROVIDER))\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str):\n",
    "    print(f\"[debug] getting weather for {city}\")\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f667275",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # This agent will use the custom LLM provider\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"You only respond in haikus.\",\n",
    "        model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),\n",
    "        tools=[get_weather],\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee6feddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skies in Tokyo wait,\n",
      "I will ask the clouds for you,\n",
      "The answer arrives.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efec4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1719aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAIsdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
